---
title: "A/B Experiments"
description: "Test prompt variations to optimize AI visibility and brand performance"
icon: "flask"
---

# Run A/B Experiments

The Experiments feature allows you to test different prompt strategies to see which performs better across AI models, helping you optimize your brand's AI presence scientifically.


## What are Experiments?

Experiments in Friction let you:

- **Test prompt variations**: Compare control vs. test prompts
- **Measure performance differences**: Track visibility, sentiment, authority, and purchase intent
- **Make data-driven decisions**: Choose the best-performing prompts based on actual AI responses
- **Optimize continuously**: Run multiple experiments to improve over time

<CardGroup cols={2}>
  <Card title="Control vs Variant Testing" icon="scale-balanced">
    Compare two sets of prompts head-to-head
  </Card>
  <Card title="Multi-Metric Tracking" icon="chart-mixed">
    Monitor up to 4 target metrics simultaneously
  </Card>
  <Card title="AI Model Selection" icon="brain">
    Test across specific LLMs and platforms
  </Card>
  <Card title="Automated Execution" icon="clock">
    Nightly job runs experiments automatically
  </Card>
</CardGroup>

<div style="background: white; border: 1px solid #e5e7eb; border-radius: 12px; padding: 24px; margin: 32px 0;">
  <div style="font-size: 13px; font-weight: 600; color: #0A054B; margin-bottom: 16px;">Experiment Results: Control vs Variant</div>
  <table style="width: 100%; border-collapse: collapse; font-size: 12px;">
    <thead>
      <tr style="border-bottom: 2px solid #e5e7eb;">
        <th style="text-align: left; padding: 12px 16px; font-weight: 600; color: #64748b; font-size: 11px; text-transform: uppercase;">Metric</th>
        <th style="text-align: center; padding: 12px 16px; font-weight: 600; color: #64748b; font-size: 11px; text-transform: uppercase;">Control</th>
        <th style="text-align: center; padding: 12px 16px; font-weight: 600; color: #64748b; font-size: 11px; text-transform: uppercase;">Variant</th>
        <th style="text-align: right; padding: 12px 16px; font-weight: 600; color: #64748b; font-size: 11px; text-transform: uppercase;">Lift</th>
      </tr>
    </thead>
    <tbody>
      <tr style="border-bottom: 1px solid #f3f4f6;">
        <td style="padding: 16px; color: #0A054B; font-weight: 600;">
          <div style="display: flex; align-items: center; gap: 8px;">
            <div style="width: 8px; height: 8px; border-radius: 50%; background: #00C4FF;"></div>
            <div>
              <div>Visibility</div>
              <div style="display: inline-block; margin-top: 4px; padding: 2px 8px; background: #dcfce7; color: #166534; font-size: 10px; font-weight: 700; border-radius: 4px;">SIGNIFICANT</div>
            </div>
          </div>
        </td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 600;">6.40</td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 700;">7.90</td>
        <td style="padding: 16px; text-align: right; color: #10b981; font-weight: 700; font-size: 14px;">+23.4%</td>
      </tr>
      <tr style="border-bottom: 1px solid #f3f4f6;">
        <td style="padding: 16px; color: #0A054B; font-weight: 600;">
          <div style="display: flex; align-items: center; gap: 8px;">
            <div style="width: 8px; height: 8px; border-radius: 50%; background: transparent; border: 2px solid #e5e7eb;"></div>
            <div>Mention Frequency</div>
          </div>
        </td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 600;">6.70</td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 700;">8.60</td>
        <td style="padding: 16px; text-align: right; color: #10b981; font-weight: 700; font-size: 14px;">+28.4%</td>
      </tr>
      <tr style="border-bottom: 1px solid #f3f4f6;">
        <td style="padding: 16px; color: #0A054B; font-weight: 600;">
          <div style="display: flex; align-items: center; gap: 8px;">
            <div style="width: 8px; height: 8px; border-radius: 50%; background: transparent; border: 2px solid #e5e7eb;"></div>
            <div>Mention Quality</div>
          </div>
        </td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 600;">7.10</td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 700;">8.30</td>
        <td style="padding: 16px; text-align: right; color: #10b981; font-weight: 700; font-size: 14px;">+16.9%</td>
      </tr>
      <tr>
        <td style="padding: 16px; color: #0A054B; font-weight: 600;">
          <div style="display: flex; align-items: center; gap: 8px;">
            <div style="width: 8px; height: 8px; border-radius: 50%; background: transparent; border: 2px solid #e5e7eb;"></div>
            <div>Authority</div>
          </div>
        </td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 600;">5.20</td>
        <td style="padding: 16px; text-align: center; color: #0A054B; font-weight: 700;">5.50</td>
        <td style="padding: 16px; text-align: right; color: #10b981; font-weight: 700; font-size: 14px;">+5.8%</td>
      </tr>
    </tbody>
  </table>
  <div style="margin-top: 16px; padding: 12px; background: #f9fafb; border-radius: 8px; font-size: 11px; color: #64748b;">
    <strong style="color: #0A054B;">Result:</strong> Variant outperformed control across all metrics. Visibility showed statistically significant improvement (+23.4%).
  </div>
</div>

---

## Experiment Lifecycle

### The 6 Experiment Statuses

<Tabs>
  <Tab title="Draft">
    **Status**: Experiment created but not started

    **What You Can Do**:
    - Edit all experiment details
    - Add or remove prompts
    - Delete the experiment
    - Save changes without starting

    **Next Step**: Click "Save & Start" when ready to begin
  </Tab>
  <Tab title="Scheduled">
    **Status**: Experiment queued for execution

    **What Happens**:
    - Nightly automation job will execute the experiment
    - Prompts sent to selected AI models
    - Results collected and analyzed
    - Status updates to "Running" during execution

    **Duration**: Remains scheduled until nightly job picks it up
  </Tab>
  <Tab title="Running">
    **Status**: Experiment currently executing

    **What Happens**:
    - AI models are responding to prompts
    - Data is being collected
    - Metrics are being calculated
    - May take hours depending on prompt count

    **View**: Real-time execution status and partial results
  </Tab>
  <Tab title="Completed">
    **Status**: Experiment finished successfully

    **What You See**:
    - Final metric scores for control and variant
    - Statistical significance
    - Winner recommendation
    - Full execution timeline
    - Detailed results breakdown

    **Next Steps**: Review results and apply insights
  </Tab>
  <Tab title="Failed">
    **Status**: Experiment encountered an error

    **What You See**:
    - Error message explaining failure
    - Failure timestamp
    - Failure reason in status badge tooltip

    **Common Causes**:
    - API timeout or rate limiting
    - Invalid prompt configuration
    - System errors

    **Resolution**: Review error, fix issues, create new experiment
  </Tab>
  <Tab title="Archived">
    **Status**: Experiment moved to archive (hidden by default)

    **Purpose**:
    - Keep experiment history clean
    - Hide old or irrelevant experiments
    - Maintain data for reference

    **View**: Toggle "Show archived" to see archived experiments

    **Note**: Can unarchive if needed
  </Tab>
</Tabs>


---

## Creating an Experiment

### Step 1: Experiment Setup

Click **"+ Create Experiment"** to open the creation form.


#### Required Fields:

<ParamField path="Experiment Name" type="string" required>
  Descriptive name for your experiment (e.g., "Product Page Prompt Test - Q1 2024")
</ParamField>

<ParamField path="Hypothesis" type="string" required>
  Your expected outcome (required for "Save & Start", optional for drafts)

  **Example**: "Benefit-focused prompts will increase purchase intent by 15% compared to feature-focused prompts"
</ParamField>

<ParamField path="Target Metrics" type="array" required>
  Select 1-4 metrics to track:
  - **Visibility**: Frequency and prominence of brand mentions
  - **Sentiment**: Positive, neutral, or negative tone
  - **Authority**: Recognition as category leader
  - **Purchase Intent**: Likelihood of AI recommendation

  **Note**: At least 1 metric must be selected
</ParamField>

#### Optional Fields:

<ParamField path="LLM Models" type="array">
  Specific AI models to test (if none selected, uses backend defaults)

  **Options**:
  - GPT-4, GPT-3.5 (OpenAI)
  - Claude 3 Opus, Sonnet, Haiku (Anthropic)
  - Gemini Pro, Ultra (Google)
  - And more...
</ParamField>

<ParamField path="AI Platforms" type="array">
  Specific AI platforms to test (if none selected, uses backend defaults)

  **Examples**: ChatGPT, Claude, Gemini, Perplexity
</ParamField>

<ParamField path="Date Range" type="object">
  Optional start and end dates for the experiment

  - **Start Date**: When experiment should begin (optional)
  - **End Date**: When experiment should conclude (optional)
  - **Validation**: End date must be after start date
</ParamField>

### Step 2: Add Prompts

Add prompts to both the **Control** and **Variant** buckets.


#### Control Bucket

The baseline prompts representing your current approach.

**Examples**:
```
"What is [Brand Name]?"
"Features of [Product]"
"How does [Product] work?"
"[Brand] specifications"
```

#### Variant Bucket

The test prompts representing your new approach.

**Examples (benefit-focused)**:
```
"How can [Brand Name] help me?"
"Benefits of [Product]"
"What problems does [Product] solve?"
"Why choose [Brand]?"
```

#### Prompt Requirements:

<Warning>
  **Minimum**: 5 prompts per bucket (10 total) required before you can "Save & Start"

  **Recommendation**: 10-20 prompts per bucket for statistically significant results
</Warning>

<Tip>
  You can save as a draft with fewer than 5 prompts per bucket, but you must add more before starting the experiment.
</Tip>

### Step 3: Save or Start

<Tabs>
  <Tab title="Save Draft">
    **What It Does**:
    - Saves experiment without starting
    - Status remains "Draft"
    - Can edit anytime
    - No minimum prompt requirement

    **Use When**:
    - Building experiment incrementally
    - Not ready to commit
    - Need approval before running
  </Tab>
  <Tab title="Save & Start">
    **What It Does**:
    1. Creates draft experiment
    2. Validates all required fields
    3. Adds prompts to both buckets
    4. Starts experiment (status â†’ "Scheduled")
    5. Queues for nightly execution

    **Requirements**:
    - Hypothesis must be filled
    - At least 1 target metric selected
    - Minimum 5 prompts per bucket (10 total)
    - Valid date range (if provided)

    **Result**: Experiment moves to "Scheduled" status and will be executed by nightly job
  </Tab>
</Tabs>

---

## Viewing Experiment Results

### Experiments List Page


#### List View Features:

<ParamField path="Filters" type="object">
  Filter experiments by status:
  - All Experiments
  - Draft
  - Scheduled
  - Running
  - Completed
  - Failed
  - Show Archived (toggle)
</ParamField>

<ParamField path="Pagination" type="object">
  Navigate large experiment lists:
  - Page size: 10, 25, 50, or 100 experiments
  - Previous/Next navigation
  - Page count display
</ParamField>

<ParamField path="Columns" type="array">
  Key information at a glance:
  - Experiment Name
  - Status (with color-coded badge)
  - Target Metrics
  - Created Date
  - Last Executed
  - Actions (View, Edit, Archive)
</ParamField>

### Experiment Detail Page

Click any experiment to view detailed results and timeline.


#### What You'll See:

<Tabs>
  <Tab title="Overview">
    - Experiment name and hypothesis
    - Current status
    - Target metrics
    - LLM models and platforms
    - Date range
    - Creation and last updated timestamps
  </Tab>
  <Tab title="Results">
    - **Control Bucket Scores**: Metrics for baseline prompts
    - **Variant Bucket Scores**: Metrics for test prompts
    - **Comparison**: Side-by-side metric comparison
    - **Winner**: AI recommendation based on performance
    - **Statistical Significance**: Confidence level in results
  </Tab>
  <Tab title="Prompt Lists">
    - Control prompts with individual results
    - Variant prompts with individual results
    - Per-prompt metric breakdowns
  </Tab>
  <Tab title="Execution Timeline">
    - Experiment created
    - Scheduled timestamp
    - Execution started
    - Execution completed (or failed)
    - Total execution time
  </Tab>
</Tabs>

---

## Understanding Results

### Metric Comparison

Results show scores for each target metric in both buckets:


#### How to Read Results:

<Tabs>
  <Tab title="Visibility">
    **Control**: 68/100
    **Variant**: 82/100
    **Lift**: +14 points (+20.6%)

    **Interpretation**: Variant prompts resulted in significantly better brand visibility

    **Action**: Adopt variant prompt style for visibility improvement
  </Tab>
  <Tab title="Sentiment">
    **Control**: 75/100
    **Variant**: 73/100
    **Lift**: -2 points (-2.7%)

    **Interpretation**: Variant slightly decreased sentiment (not statistically significant)

    **Action**: Monitor sentiment if adopting variant
  </Tab>
  <Tab title="Authority">
    **Control**: 55/100
    **Variant**: 62/100
    **Lift**: +7 points (+12.7%)

    **Interpretation**: Variant improved authority perception

    **Action**: Variant helps establish category leadership
  </Tab>
  <Tab title="Purchase Intent">
    **Control**: 60/100
    **Variant**: 78/100
    **Lift**: +18 points (+30%)

    **Interpretation**: Variant dramatically increased purchase recommendations

    **Action**: Strong win - adopt variant for commerce optimization
  </Tab>
</Tabs>

### Statistical Significance

<Info>
  **Confidence Level**: Results are considered statistically significant if the confidence level exceeds 95%.

  **Sample Size**: Larger prompt sets (20+ per bucket) yield more reliable results.
</Info>

#### Interpreting Significance:

<AccordionGroup>
  <Accordion title="High Confidence (95%+)">
    **Meaning**: Very likely the observed difference is real, not random chance

    **Action**: Confidently implement the winning variant
  </Accordion>
  <Accordion title="Medium Confidence (80-95%)">
    **Meaning**: Likely a real difference, but some uncertainty remains

    **Action**: Consider running a larger follow-up experiment or implement cautiously
  </Accordion>
  <Accordion title="Low Confidence (<80%)">
    **Meaning**: Observed differences may be due to chance

    **Action**: Run longer experiment with more prompts, or interpret results as inconclusive
  </Accordion>
</AccordionGroup>

### Winner Determination

Friction automatically recommends a winner based on:

1. **Primary Metric Performance**: Largest improvement in target metrics
2. **Statistical Significance**: Confidence level of results
3. **No Negative Impacts**: Guard rail metrics don't decline significantly
4. **Consistency**: Performance across different AI models


---

## Best Practices

### Designing Good Experiments

<Steps>
  <Step title="Test One Variable">
    Change one thing between control and variant for clear causality

    **Good**: Control uses features, Variant uses benefits

    **Bad**: Control uses features + formal tone, Variant uses benefits + casual tone (which caused the difference?)
  </Step>
  <Step title="Use Sufficient Sample Size">
    Aim for 10-20 prompts per bucket minimum

    **Why**: Larger samples reduce noise and increase confidence
  </Step>
  <Step title="Run for Appropriate Duration">
    Allow nightly job to execute multiple times if needed

    **Recommendation**: Let experiments run for at least 7 days for AI model caching to settle
  </Step>
  <Step title="Align with Business Goals">
    Choose target metrics that matter to your business

    **E-Commerce**: Prioritize Purchase Intent

    **B2B SaaS**: Prioritize Authority

    **Consumer Brand**: Prioritize Sentiment
  </Step>
</Steps>

### Experiment Ideas

<Tabs>
  <Tab title="Prompt Framing">
    **Control**: "What is [Product]?"

    **Variant**: "How can [Product] help me?"

    **Tests**: Question framing impact on recommendations
  </Tab>
  <Tab title="Feature vs Benefit">
    **Control**: "Features of [Product]"

    **Variant**: "Benefits of using [Product]"

    **Tests**: Whether benefits drive better sentiment/purchase intent
  </Tab>
  <Tab title="Comparison Positioning">
    **Control**: "[Brand] vs [Competitor]"

    **Variant**: "Why choose [Brand] over [Competitor]?"

    **Tests**: Assertive positioning impact
  </Tab>
  <Tab title="Specificity">
    **Control**: "Best project management software"

    **Variant**: "Best project management software for remote teams"

    **Tests**: Niche targeting vs broad queries
  </Tab>
  <Tab title="Problem-Solution">
    **Control**: "What does [Product] do?"

    **Variant**: "How does [Product] solve [problem]?"

    **Tests**: Problem-solution framing effectiveness
  </Tab>
</Tabs>

### Common Mistakes to Avoid

<Warning>
  **Don't end experiments prematurely**. Wait for full execution and statistical significance before drawing conclusions.
</Warning>

<AccordionGroup>
  <Accordion title="Testing Too Many Variables">
    **Problem**: Can't identify which change caused the difference

    **Solution**: Test one major variable at a time
  </Accordion>
  <Accordion title="Insufficient Prompts">
    **Problem**: Results lack statistical power

    **Solution**: Use 10-20 prompts per bucket minimum
  </Accordion>
  <Accordion title="Ignoring Negative Results">
    **Problem**: Only implementing winning experiments misses learnings

    **Solution**: Document why variants failed - equally valuable insight
  </Accordion>
  <Accordion title="Not Aligning with Strategy">
    **Problem**: Running random experiments without clear goals

    **Solution**: Tie experiments to specific business objectives
  </Accordion>
  <Accordion title="Forgetting to Implement Winners">
    **Problem**: Running experiments but not applying insights

    **Solution**: Create action plan to implement winning prompts across Define page
  </Accordion>
</AccordionGroup>

---

## Managing Experiments

### Editing Experiments

<Tabs>
  <Tab title="Draft Experiments">
    **Can Edit**: Everything
    - Name, hypothesis
    - Target metrics
    - LLM models, platforms
    - Date range
    - Add/remove/edit prompts

    **How**: Click "Edit" on experiments list or detail page
  </Tab>
  <Tab title="Scheduled/Running">
    **Can Edit**: Limited
    - Cannot edit experiment details
    - Cannot change prompts
    - Can archive or delete (if needed)

    **Note**: Create new experiment if you need changes
  </Tab>
  <Tab title="Completed">
    **Can Edit**: Metadata only
    - Can update name or hypothesis for documentation
    - Cannot change prompts or metrics (would invalidate results)
    - Can archive if no longer relevant
  </Tab>
</Tabs>

### Archiving Experiments

Archive old or irrelevant experiments to keep your list clean:

<Steps>
  <Step title="Select Experiment">
    Click the archive icon on experiments list or detail page
  </Step>
  <Step title="Confirm Archive">
    Experiment moves to archived status
  </Step>
  <Step title="View Archived">
    Toggle "Show archived" filter to see archived experiments
  </Step>
  <Step title="Unarchive (if needed)">
    Click unarchive icon to restore experiment to active list
  </Step>
</Steps>

<Info>
  **Archiving vs Deleting**: Archiving preserves experiment data for future reference. Deleting permanently removes the experiment.
</Info>

### Deleting Experiments

<Warning>
  Deleting an experiment is permanent and cannot be undone. All data will be lost.
</Warning>

**When to Delete**:
- Test experiments during onboarding
- Duplicate experiments created by mistake
- Experiments with critical errors in setup
- Data you're certain you'll never need

**When to Archive Instead**:
- Completed experiments (preserve historical data)
- Experiments with learnings to reference later
- Anything you might want to review in the future

---

## Experiment Workflow Example

### Real-World Scenario: E-Commerce Brand

**Goal**: Increase purchase intent for flagship product

<Steps>
  <Step title="Hypothesis Formation">
    **Hypothesis**: "Benefit-focused prompts emphasizing problem-solving will increase purchase intent by 20% compared to feature-focused prompts"

    **Reasoning**: Customers care more about outcomes than specifications
  </Step>
  <Step title="Create Experiment">
    - Name: "Product Page Prompts - Benefit vs Feature Test"
    - Target Metrics: Purchase Intent (primary), Sentiment (secondary)
    - LLM Models: GPT-4, Claude 3, Gemini Pro
  </Step>
  <Step title="Build Control Bucket (Feature-Focused)">
    ```
    "What are the features of [Product]?"
    "Specifications of [Product]"
    "Technical details of [Product]"
    "[Product] capabilities"
    "What does [Product] include?"
    ... (10 total feature-focused prompts)
    ```
  </Step>
  <Step title="Build Variant Bucket (Benefit-Focused)">
    ```
    "How can [Product] help me save time?"
    "What problems does [Product] solve?"
    "Benefits of using [Product]"
    "Why should I choose [Product]?"
    "How does [Product] make life easier?"
    ... (10 total benefit-focused prompts)
    ```
  </Step>
  <Step title="Save & Start">
    Experiment moves to "Scheduled" status, queued for nightly execution
  </Step>
  <Step title="Monitor Results">
    After execution completes:
    - **Control Purchase Intent**: 62/100
    - **Variant Purchase Intent**: 79/100
    - **Lift**: +17 points (+27.4%)
    - **Confidence**: 96%

    **Winner**: Variant (benefit-focused prompts)
  </Step>
  <Step title="Apply Insights">
    - Update Define page prompts to use benefit-focused language
    - Apply benefit framing to product pages
    - Run follow-up experiment testing different benefit themes
  </Step>
</Steps>

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Experiment Stuck in Scheduled">
    **Possible Causes**:
    - Nightly job hasn't run yet (wait 24 hours)
    - Queue backlog (multiple experiments scheduled)
    - System issue

    **Solution**: Wait for next nightly execution window. Contact support if stuck >48 hours.
  </Accordion>
  <Accordion title="Experiment Failed">
    **Check**: Failure reason in status badge tooltip or detail page

    **Common Causes**:
    - Invalid prompts (empty, malformed)
    - API rate limiting
    - Timeout due to too many prompts

    **Solution**: Fix issue, create new experiment
  </Accordion>
  <Accordion title="Results Show No Difference">
    **Interpretation**: Control and variant performed similarly

    **Possible Reasons**:
    - Tested variable doesn't matter for this metric
    - Change was too subtle
    - Sample size too small

    **Next Steps**: Try more dramatic variation or different test variable
  </Accordion>
  <Accordion title="Can't Start Experiment">
    **Check**:
    - Hypothesis filled in
    - At least 1 target metric selected
    - Minimum 5 prompts per bucket
    - Valid date range (end > start)

    **Solution**: Address missing requirements
  </Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Define Prompts"
    icon="pen-to-square"
    href="/features/define"
  >
    Apply experiment insights to your prompt library
  </Card>
  <Card
    title="Analyze Results"
    icon="chart-mixed"
    href="/features/analyze"
  >
    See how experiments impact overall brand health
  </Card>
  <Card
    title="View Dashboard"
    icon="gauge"
    href="/dashboard/overview"
  >
    Monitor impact of implemented changes
  </Card>
  <Card
    title="Commerce Tracking"
    icon="cart-shopping"
    href="/features/commerce"
  >
    Run commerce-specific experiments
  </Card>
</CardGroup>

---

<Note>
  **Pro Tip**: Start with high-impact, low-effort experiments. Test prompt framing or feature vs. benefit messaging first - these often show significant results with minimal effort. Save complex multivariate tests for later when you have more experience.
</Note>
