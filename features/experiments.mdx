---
title: "A/B Experiments"
description: "Test prompt variations to optimize AI visibility and brand performance"
icon: "flask"
---

# Run A/B Experiments

The Experiments feature allows you to test different prompt strategies to see which performs better across AI models, helping you optimize your brand's AI presence scientifically.

<Info>
  **Professional tier and above**: Experiments are available on Professional and Enterprise plans. [View pricing](https://frictionai.co/pricing) for details.
</Info>

## What are Experiments?

Experiments in friction let you:

- **Test prompt variations**: Compare control vs. test prompts
- **Measure visibility performance**: Track how prompt changes affect brand visibility
- **Make data-driven decisions**: Choose the best-performing prompts based on actual AI responses
- **Optimize continuously**: Run multiple experiments to improve over time

<CardGroup cols={2}>
  <Card title="Control vs Variant Testing" icon="scale-balanced">
    Compare two sets of prompts head-to-head
  </Card>
  <Card title="Visibility Tracking" icon="chart-mixed">
    Monitor visibility performance across AI models
  </Card>
  <Card title="AI Model Selection" icon="brain">
    Test across specific LLMs and platforms
  </Card>
  <Card title="Automated Execution" icon="clock">
    Nightly job runs experiments automatically
  </Card>
</CardGroup>

<Frame>
  <img src="/images/experiments-results.svg" alt="A/B experiment results comparing control vs variant across multiple metrics with statistical significance indicators" />
</Frame>

---

## Experiment Lifecycle

### The 6 Experiment Statuses

<Tabs>
  <Tab title="Draft">
    **Status**: Experiment created but not started

    **What You Can Do**:
    - Edit all experiment details
    - Add or remove prompts
    - Delete the experiment
    - Save changes without starting

    **Next Step**: Click "Save & Start" when ready to begin
  </Tab>
  <Tab title="Scheduled">
    **Status**: Experiment queued for execution

    **What Happens**:
    - Nightly automation job will execute the experiment
    - Prompts sent to selected AI models
    - Results collected and analyzed
    - Status updates to "Running" during execution

    **Duration**: Remains scheduled until nightly job picks it up
  </Tab>
  <Tab title="Running">
    **Status**: Experiment currently executing

    **What Happens**:
    - AI models are responding to prompts
    - Data is being collected
    - Metrics are being calculated
    - May take hours depending on prompt count

    **View**: Real-time execution status and partial results
  </Tab>
  <Tab title="Completed">
    **Status**: Experiment finished successfully

    **What You See**:
    - Final metric scores for control and variant
    - Statistical significance
    - Winner recommendation
    - Full execution timeline
    - Detailed results breakdown

    **Next Steps**: Review results and apply insights
  </Tab>
  <Tab title="Failed">
    **Status**: Experiment encountered an error

    **What You See**:
    - Error message explaining failure
    - Failure timestamp
    - Failure reason in status badge tooltip

    **Common Causes**:
    - API timeout or rate limiting
    - Invalid prompt configuration
    - System errors

    **Resolution**: Review error, fix issues, create new experiment
  </Tab>
  <Tab title="Archived">
    **Status**: Experiment moved to archive (hidden by default)

    **Purpose**:
    - Keep experiment history clean
    - Hide old or irrelevant experiments
    - Maintain data for reference

    **View**: Toggle "Show archived" to see archived experiments

    **Note**: Can unarchive if needed
  </Tab>
</Tabs>


---

## Creating an Experiment

### Step 1: Experiment Setup

Click **"+ Create Experiment"** to open the creation form.


#### Required Fields:

<ParamField path="Experiment Name" type="string" required>
  Descriptive name for your experiment (e.g., "Product Page Prompt Test - Q1 2024")
</ParamField>

<ParamField path="Hypothesis" type="string" required>
  Your expected outcome (required for "Save & Start", optional for drafts)

  **Example**: "Benefit-focused prompts will increase purchase intent by 15% compared to feature-focused prompts"
</ParamField>

<ParamField path="Target Metric" type="string" required>
  Experiments track **Visibility**: Frequency and prominence of brand mentions across AI responses.

  **Note**: Visibility is the primary metric for A/B experiments, measuring how often your brand appears in AI-generated responses.
</ParamField>

#### Optional Fields:

<ParamField path="LLM Models" type="array">
  Specific AI models to test (if none selected, uses backend defaults)

  **Options**:
  - GPT-4, GPT-3.5 (OpenAI)
  - Claude 3 Opus, Sonnet, Haiku (Anthropic)
  - Gemini Pro, Ultra (Google)
  - And more...
</ParamField>

<ParamField path="AI Platforms" type="array">
  Specific AI platforms to test (if none selected, uses backend defaults)

  **Examples**: ChatGPT, Claude, Gemini, Perplexity
</ParamField>

<ParamField path="Date Range" type="object">
  Optional start and end dates for the experiment

  - **Start Date**: When experiment should begin (optional)
  - **End Date**: When experiment should conclude (optional)
  - **Validation**: End date must be after start date
</ParamField>

### Step 2: Add Prompts

Add prompts to both the **Control** and **Variant** buckets.


#### Control Bucket

The baseline prompts representing your current approach.

**Examples**:
```
"What is [Brand Name]?"
"Features of [Product]"
"How does [Product] work?"
"[Brand] specifications"
```

#### Variant Bucket

The test prompts representing your new approach.

**Examples (benefit-focused)**:
```
"How can [Brand Name] help me?"
"Benefits of [Product]"
"What problems does [Product] solve?"
"Why choose [Brand]?"
```

#### Prompt Requirements:

<Warning>
  **Minimum**: 5 prompts per bucket (10 total) required before you can "Save & Start"

  **Recommendation**: 10-20 prompts per bucket for statistically significant results
</Warning>

<Tip>
  You can save as a draft with fewer than 5 prompts per bucket, but you must add more before starting the experiment.
</Tip>

### Step 3: Save or Start

<Tabs>
  <Tab title="Save Draft">
    **What It Does**:
    - Saves experiment without starting
    - Status remains "Draft"
    - Can edit anytime
    - No minimum prompt requirement

    **Use When**:
    - Building experiment incrementally
    - Not ready to commit
    - Need approval before running
  </Tab>
  <Tab title="Save & Start">
    **What It Does**:
    1. Creates draft experiment
    2. Validates all required fields
    3. Adds prompts to both buckets
    4. Starts experiment (status â†’ "Scheduled")
    5. Queues for nightly execution

    **Requirements**:
    - Hypothesis must be filled
    - Minimum 5 prompts per bucket (10 total)
    - Valid date range (if provided)

    **Result**: Experiment moves to "Scheduled" status and will be executed by nightly job
  </Tab>
</Tabs>

---

## Viewing Experiment Results

### Experiments List Page


#### List View Features:

<ParamField path="Filters" type="object">
  Filter experiments by status:
  - All Experiments
  - Draft
  - Scheduled
  - Running
  - Completed
  - Failed
  - Show Archived (toggle)
</ParamField>

<ParamField path="Pagination" type="object">
  Navigate large experiment lists:
  - Page size: 10, 25, 50, or 100 experiments
  - Previous/Next navigation
  - Page count display
</ParamField>

<ParamField path="Columns" type="array">
  Key information at a glance:
  - Experiment Name
  - Status (with color-coded badge)
  - Target Metrics
  - Created Date
  - Last Executed
  - Actions (View, Edit, Archive)
</ParamField>

### Experiment Detail Page

Click any experiment to view detailed results and timeline.


#### What You'll See:

<Tabs>
  <Tab title="Overview">
    - Experiment name and hypothesis
    - Current status
    - Target metrics
    - LLM models and platforms
    - Date range
    - Creation and last updated timestamps
  </Tab>
  <Tab title="Results">
    - **Control Bucket Scores**: Metrics for baseline prompts
    - **Variant Bucket Scores**: Metrics for test prompts
    - **Comparison**: Side-by-side metric comparison
    - **Winner**: AI recommendation based on performance
    - **Statistical Significance**: Confidence level in results
  </Tab>
  <Tab title="Prompt Lists">
    - Control prompts with individual results
    - Variant prompts with individual results
    - Per-prompt metric breakdowns
  </Tab>
  <Tab title="Execution Timeline">
    - Experiment created
    - Scheduled timestamp
    - Execution started
    - Execution completed (or failed)
    - Total execution time
  </Tab>
</Tabs>

---

## Understanding Results

### Metric Comparison

Results show scores for each target metric in both buckets:


#### How to Read Results:

**Visibility Comparison**:
- **Control**: 68/100
- **Variant**: 82/100
- **Lift**: +14 points (+20.6%)

**Interpretation**: Variant prompts resulted in significantly better brand visibility across AI models.

**Action**: Adopt variant prompt style for visibility improvement. The experiment shows that the new prompt approach leads to your brand appearing more frequently and prominently in AI responses.

### Statistical Significance

<Info>
  **Confidence Level**: Results are considered statistically significant if the confidence level exceeds 95%.

  **Sample Size**: Larger prompt sets (20+ per bucket) yield more reliable results.
</Info>

#### Interpreting Significance:

<AccordionGroup>
  <Accordion title="High Confidence (95%+)">
    **Meaning**: Very likely the observed difference is real, not random chance

    **Action**: Confidently implement the winning variant
  </Accordion>
  <Accordion title="Medium Confidence (80-95%)">
    **Meaning**: Likely a real difference, but some uncertainty remains

    **Action**: Consider running a larger follow-up experiment or implement cautiously
  </Accordion>
  <Accordion title="Low Confidence (<80%)">
    **Meaning**: Observed differences may be due to chance

    **Action**: Run longer experiment with more prompts, or interpret results as inconclusive
  </Accordion>
</AccordionGroup>

### Winner Determination

friction automatically recommends a winner based on:

1. **Primary Metric Performance**: Largest improvement in target metrics
2. **Statistical Significance**: Confidence level of results
3. **No Negative Impacts**: Guard rail metrics don't decline significantly
4. **Consistency**: Performance across different AI models


---

## Best Practices

### Designing Good Experiments

<Steps>
  <Step title="Test One Variable">
    Change one thing between control and variant for clear causality

    **Good**: Control uses features, Variant uses benefits

    **Bad**: Control uses features + formal tone, Variant uses benefits + casual tone (which caused the difference?)
  </Step>
  <Step title="Use Sufficient Sample Size">
    Aim for 10-20 prompts per bucket minimum

    **Why**: Larger samples reduce noise and increase confidence
  </Step>
  <Step title="Run for Appropriate Duration">
    Allow nightly job to execute multiple times if needed

    **Recommendation**: Let experiments run for at least 7 days for AI model caching to settle
  </Step>
  <Step title="Align with Business Goals">
    Focus on prompt strategies that improve visibility for your business context

    **E-Commerce**: Test product-focused prompts
    **B2B SaaS**: Test expertise and solution prompts
    **Consumer Brand**: Test brand awareness prompts
  </Step>
</Steps>

### Experiment Ideas

<Tabs>
  <Tab title="Prompt Framing">
    **Control**: "What is [Product]?"

    **Variant**: "How can [Product] help me?"

    **Tests**: Question framing impact on recommendations
  </Tab>
  <Tab title="Feature vs Benefit">
    **Control**: "Features of [Product]"

    **Variant**: "Benefits of using [Product]"

    **Tests**: Whether benefits drive better sentiment/purchase intent
  </Tab>
  <Tab title="Comparison Positioning">
    **Control**: "[Brand] vs [Competitor]"

    **Variant**: "Why choose [Brand] over [Competitor]?"

    **Tests**: Assertive positioning impact
  </Tab>
  <Tab title="Specificity">
    **Control**: "Best project management software"

    **Variant**: "Best project management software for remote teams"

    **Tests**: Niche targeting vs broad queries
  </Tab>
  <Tab title="Problem-Solution">
    **Control**: "What does [Product] do?"

    **Variant**: "How does [Product] solve [problem]?"

    **Tests**: Problem-solution framing effectiveness
  </Tab>
</Tabs>

### Common Mistakes to Avoid

<Warning>
  **Don't end experiments prematurely**. Wait for full execution and statistical significance before drawing conclusions.
</Warning>

<AccordionGroup>
  <Accordion title="Testing Too Many Variables">
    **Problem**: Can't identify which change caused the difference

    **Solution**: Test one major variable at a time
  </Accordion>
  <Accordion title="Insufficient Prompts">
    **Problem**: Results lack statistical power

    **Solution**: Use 10-20 prompts per bucket minimum
  </Accordion>
  <Accordion title="Ignoring Negative Results">
    **Problem**: Only implementing winning experiments misses learnings

    **Solution**: Document why variants failed - equally valuable insight
  </Accordion>
  <Accordion title="Not Aligning with Strategy">
    **Problem**: Running random experiments without clear goals

    **Solution**: Tie experiments to specific business objectives
  </Accordion>
  <Accordion title="Forgetting to Implement Winners">
    **Problem**: Running experiments but not applying insights

    **Solution**: Create action plan to implement winning prompts across Visibility page
  </Accordion>
</AccordionGroup>

---

## Managing Experiments

### Editing Experiments

<Tabs>
  <Tab title="Draft Experiments">
    **Can Edit**: Everything
    - Name, hypothesis
    - Target metrics
    - LLM models, platforms
    - Date range
    - Add/remove/edit prompts

    **How**: Click "Edit" on experiments list or detail page
  </Tab>
  <Tab title="Scheduled/Running">
    **Can Edit**: Limited
    - Cannot edit experiment details
    - Cannot change prompts
    - Can archive or delete (if needed)

    **Note**: Create new experiment if you need changes
  </Tab>
  <Tab title="Completed">
    **Can Edit**: Metadata only
    - Can update name or hypothesis for documentation
    - Cannot change prompts or metrics (would invalidate results)
    - Can archive if no longer relevant
  </Tab>
</Tabs>

### Archiving Experiments

Archive old or irrelevant experiments to keep your list clean:

<Steps>
  <Step title="Select Experiment">
    Click the archive icon on experiments list or detail page
  </Step>
  <Step title="Confirm Archive">
    Experiment moves to archived status
  </Step>
  <Step title="View Archived">
    Toggle "Show archived" filter to see archived experiments
  </Step>
  <Step title="Unarchive (if needed)">
    Click unarchive icon to restore experiment to active list
  </Step>
</Steps>

<Info>
  **Archiving vs Deleting**: Archiving preserves experiment data for future reference. Deleting permanently removes the experiment.
</Info>

### Deleting Experiments

<Warning>
  Deleting an experiment is permanent and cannot be undone. All data will be lost.
</Warning>

**When to Delete**:
- Test experiments during onboarding
- Duplicate experiments created by mistake
- Experiments with critical errors in setup
- Data you're certain you'll never need

**When to Archive Instead**:
- Completed experiments (preserve historical data)
- Experiments with learnings to reference later
- Anything you might want to review in the future

---

## Experiment Workflow Example

### Real-World Scenario: E-Commerce Brand

**Goal**: Increase purchase intent for flagship product

<Steps>
  <Step title="Hypothesis Formation">
    **Hypothesis**: "Benefit-focused prompts emphasizing problem-solving will increase purchase intent by 20% compared to feature-focused prompts"

    **Reasoning**: Customers care more about outcomes than specifications
  </Step>
  <Step title="Create Experiment">
    - Name: "Product Page Prompts - Benefit vs Feature Test"
    - Target Metric: Visibility
    - LLM Models: GPT-4, Claude 3, Gemini Pro
  </Step>
  <Step title="Build Control Bucket (Feature-Focused)">
    ```
    "What are the features of [Product]?"
    "Specifications of [Product]"
    "Technical details of [Product]"
    "[Product] capabilities"
    "What does [Product] include?"
    ... (10 total feature-focused prompts)
    ```
  </Step>
  <Step title="Build Variant Bucket (Benefit-Focused)">
    ```
    "How can [Product] help me save time?"
    "What problems does [Product] solve?"
    "Benefits of using [Product]"
    "Why should I choose [Product]?"
    "How does [Product] make life easier?"
    ... (10 total benefit-focused prompts)
    ```
  </Step>
  <Step title="Save & Start">
    Experiment moves to "Scheduled" status, queued for nightly execution
  </Step>
  <Step title="Monitor Results">
    After execution completes:
    - **Control Visibility**: 62/100
    - **Variant Visibility**: 79/100
    - **Lift**: +17 points (+27.4%)
    - **Confidence**: 96%

    **Winner**: Variant (benefit-focused prompts)
  </Step>
  <Step title="Apply Insights">
    - Update Visibility page prompts to use benefit-focused language
    - Apply benefit framing to product pages
    - Run follow-up experiment testing different benefit themes
  </Step>
</Steps>

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="Experiment Stuck in Scheduled">
    **Possible Causes**:
    - Nightly job hasn't run yet (wait 24 hours)
    - Queue backlog (multiple experiments scheduled)
    - System issue

    **Solution**: Wait for next nightly execution window. Contact support if stuck >48 hours.
  </Accordion>
  <Accordion title="Experiment Failed">
    **Check**: Failure reason in status badge tooltip or detail page

    **Common Causes**:
    - Invalid prompts (empty, malformed)
    - API rate limiting
    - Timeout due to too many prompts

    **Solution**: Fix issue, create new experiment
  </Accordion>
  <Accordion title="Results Show No Difference">
    **Interpretation**: Control and variant performed similarly

    **Possible Reasons**:
    - Tested variable doesn't matter for this metric
    - Change was too subtle
    - Sample size too small

    **Next Steps**: Try more dramatic variation or different test variable
  </Accordion>
  <Accordion title="Can't Start Experiment">
    **Check**:
    - Hypothesis filled in
    - Minimum 5 prompts per bucket
    - Valid date range (end > start)

    **Solution**: Address missing requirements
  </Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Define Prompts"
    icon="pen-to-square"
    href="/features/define"
  >
    Apply experiment insights to your prompt library
  </Card>
  <Card
    title="Analyze Results"
    icon="chart-mixed"
    href="/features/analyze"
  >
    See how experiments impact overall brand health
  </Card>
  <Card
    title="View Dashboard"
    icon="gauge"
    href="/dashboard/overview"
  >
    Monitor impact of implemented changes
  </Card>
  <Card
    title="Commerce Tracking"
    icon="cart-shopping"
    href="/features/commerce"
  >
    Run commerce-specific experiments
  </Card>
</CardGroup>

---

<Note>
  **Pro Tip**: Start with high-impact, low-effort experiments. Test prompt framing or feature vs. benefit messaging first - these often show significant results with minimal effort. Save complex multivariate tests for later when you have more experience.
</Note>
